#!/usr/bin/env python3
"""
é‡‘ç®¡æœƒæ™ºèƒ½å•ç­”ç³»çµ± - Streamlit éƒ¨ç½²ç‰ˆæœ¬

æ”¯æ´ä¸‰ç¨®è³‡æ–™ä¾†æºï¼š
- è£ç½°æ¡ˆä»¶
- æ³•ä»¤å‡½é‡‹
- é‡è¦å…¬å‘Š
"""

import streamlit as st
import os
import time
import json
from typing import List, Dict, Any
from pathlib import Path

# é é¢é…ç½®
st.set_page_config(
    page_title="é‡‘ç®¡æœƒæ™ºèƒ½å•ç­”",
    page_icon="ğŸ›ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Store é…ç½®
STORES = {
    'penalties': {
        'name': 'fsc-penalties-plaintext',
        'store_id': 'fileSearchStores/fscpenaltiesplaintext-4f87t5uexgui',
        'display_name': 'è£ç½°æ¡ˆä»¶',
        'icon': 'âš–ï¸',
        'description': '490 ç­†é‡‘èæ©Ÿæ§‹è£ç½°æ¡ˆä»¶ (2012-2025)',
        'count': 490,
    },
    'law_interpretations': {
        'name': 'fsc-law-interpretations',
        'store_id': 'fileSearchStores/fsclawinterpretations-zz5pwrly06hz',
        'display_name': 'æ³•ä»¤å‡½é‡‹',
        'icon': 'ğŸ“œ',
        'description': 'æ³•è¦è§£é‡‹ã€ä¿®æ­£èªªæ˜ã€æ¢æ–‡å°ç…§',
        'count': 2872,
    },
    'announcements': {
        'name': 'fsc-announcements',
        'store_id': 'fileSearchStores/fscannouncements-o94q0kmo2zxb',
        'display_name': 'é‡è¦å…¬å‘Š',
        'icon': 'ğŸ“¢',
        'description': 'æ”¿ç­–å…¬å‘Šã€æ³•è¦ä¿®æ­£å…¬å‘Š',
        'count': 1642,
    },
}

# è¼‰å…¥ Mapping æª”æ¡ˆ
def load_mappings():
    """
    è¼‰å…¥æ‰€æœ‰è³‡æ–™é¡å‹çš„ gemini_id_mapping å’Œ file_mapping æª”æ¡ˆ
    ç”¨æ–¼å°‡ Gemini å›å‚³çš„ file ID è½‰æ›ç‚ºå¯è®€çš„é¡¯ç¤ºåç¨±
    """
    data_path = Path(__file__).parent.parent / "data"

    gemini_id_mapping = {}  # gemini_short_id â†’ doc_id
    file_mapping = {}       # doc_id â†’ info

    try:
        # === è¼‰å…¥è£ç½°æ¡ˆä»¶ (èˆŠæ ¼å¼) ===
        penalties_path = data_path / "penalties"

        # gemini_id_mapping.json: {files/xxx: doc_id}
        gemini_mapping_path = penalties_path / "gemini_id_mapping.json"
        if gemini_mapping_path.exists():
            with open(gemini_mapping_path, 'r', encoding='utf-8') as f:
                raw_mapping = json.load(f)
                for full_id, doc_id in raw_mapping.items():
                    short_id = full_id.replace('files/', '')
                    gemini_id_mapping[short_id] = doc_id

        # file_mapping.json: {doc_id: info}
        file_mapping_path = penalties_path / "file_mapping.json"
        if file_mapping_path.exists():
            with open(file_mapping_path, 'r', encoding='utf-8') as f:
                file_mapping.update(json.load(f))

        # === è¼‰å…¥æ³•ä»¤å‡½é‡‹ ===
        law_path = data_path / "law_interpretations"
        law_gemini_path = law_path / "gemini_id_mapping_new.json"
        if law_gemini_path.exists():
            with open(law_gemini_path, 'r', encoding='utf-8') as f:
                raw_mapping = json.load(f)
                for doc_id, info in raw_mapping.items():
                    gemini_file_id = info.get('gemini_file_id', '')
                    if gemini_file_id:
                        short_id = gemini_file_id.replace('files/', '')
                        gemini_id_mapping[short_id] = doc_id
                    file_mapping[doc_id] = {
                        'display_name': info.get('display_name', ''),
                        'date': info.get('date', ''),
                        'source': info.get('source', ''),
                        'category': info.get('category', ''),
                        'original_url': info.get('original_url', ''),
                    }

        # === è¼‰å…¥é‡è¦å…¬å‘Š ===
        ann_path = data_path / "announcements"
        ann_gemini_path = ann_path / "gemini_id_mapping_new.json"
        if ann_gemini_path.exists():
            with open(ann_gemini_path, 'r', encoding='utf-8') as f:
                raw_mapping = json.load(f)
                for doc_id, info in raw_mapping.items():
                    gemini_file_id = info.get('gemini_file_id', '')
                    if gemini_file_id:
                        short_id = gemini_file_id.replace('files/', '')
                        gemini_id_mapping[short_id] = doc_id
                    file_mapping[doc_id] = {
                        'display_name': info.get('display_name', ''),
                        'date': info.get('date', ''),
                        'source': info.get('source', ''),
                        'category': info.get('category', ''),
                        'original_url': info.get('original_url', ''),
                    }

    except Exception as e:
        st.warning(f"è¼‰å…¥ mapping æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return gemini_id_mapping, file_mapping

# å…¨åŸŸ Mapping (è¼‰å…¥ä¸€æ¬¡)
GEMINI_ID_MAPPING, FILE_MAPPING = load_mappings()


def resolve_source_display_name(raw_id: str) -> tuple:
    """
    å°‡ Gemini å›å‚³çš„ file ID è§£æç‚ºå¯è®€çš„é¡¯ç¤ºåç¨±

    å›å‚³: (display_name, source_type, date, original_url)
    """
    # å˜—è©¦å¾ mapping æŸ¥è©¢
    doc_id = GEMINI_ID_MAPPING.get(raw_id, '')

    if doc_id and doc_id in FILE_MAPPING:
        info = FILE_MAPPING[doc_id]
        display_name = info.get('display_name', '')
        date = info.get('date', 'æœªçŸ¥æ—¥æœŸ')
        source = info.get('source', '')
        original_url = info.get('original_url', '')

        # åˆ¤æ–·ä¾†æºé¡å‹
        if doc_id.startswith('fsc_pen'):
            source_type = "è£ç½°æ¡ˆä»¶"
            icon = "âš–ï¸"
        elif doc_id.startswith('fsc_law'):
            source_type = "æ³•ä»¤å‡½é‡‹"
            icon = "ğŸ“œ"
        elif doc_id.startswith('fsc_unk') or doc_id.startswith('fsc_ann'):
            source_type = "é‡è¦å…¬å‘Š"
            icon = "ğŸ“¢"
        else:
            source_type = "æœªçŸ¥"
            icon = "ğŸ“„"

        # ä¾†æºå–®ä½ä¸­æ–‡åŒ–
        source_map = {
            'insurance_bureau': 'ä¿éšªå±€',
            'securities_bureau': 'è­‰æœŸå±€',
            'bank_bureau': 'éŠ€è¡Œå±€',
            'fsc': 'é‡‘ç®¡æœƒ',
        }
        source_display = source_map.get(source, source)

        # æ ¼å¼åŒ–é¡¯ç¤ºåç¨±
        if display_name:
            # è£ç½°æ¡ˆä»¶æ ¼å¼: "2025-09-25_ä¿éšªå±€_å…¨çƒäººå£½"
            # æ–°æ ¼å¼: "2025-11-14_insurance_bureau_ann_amendment_fsc_unk_..."
            parts = display_name.split('_')
            if doc_id.startswith('fsc_pen') and len(parts) >= 3:
                # è£ç½°: æ—¥æœŸ_ä¾†æº_æ©Ÿæ§‹åç¨±
                return f"{icon} {parts[0]}_{parts[2]}", source_type, date, original_url
            elif len(parts) >= 2:
                # æ³•ä»¤å‡½é‡‹/å…¬å‘Š: æ—¥æœŸ_ä¾†æº
                return f"{icon} {date}_{source_display}", source_type, date, original_url

        return f"{icon} {source_type}_{date}", source_type, date, original_url

    # å¦‚æœ mapping æ‰¾ä¸åˆ°ï¼Œå˜—è©¦å¾åŸå§‹åç¨±è§£æ
    return f"ğŸ“„ {format_source_display_name(raw_id)}", "æœªçŸ¥", "æœªçŸ¥æ—¥æœŸ", ""


# ç¯„ä¾‹å•é¡Œ
EXAMPLE_QUESTIONS = [
    "é•åé‡‘æ§æ³•åˆ©å®³é—œä¿‚äººè¦å®šæœƒå—åˆ°ä»€éº¼è™•ç½°ï¼Ÿ",
    "è«‹å•åœ¨è­‰åˆ¸å› ç‚ºå°ˆæ¥­æŠ•è³‡äººè³‡æ ¼å¯©æ ¸çš„è£ç½°æœ‰å“ªäº›ï¼Ÿ",
    "è¾¦ç†å…±åŒè¡ŒéŠ·è¢«è£ç½°çš„æ¡ˆä¾‹æœ‰å“ªäº›ï¼Ÿ",
    "é‡‘ç®¡æœƒå°å‰µæŠ•å…¬å¸çš„è£ç½°æœ‰å“ªäº›ï¼Ÿ",
    "è­‰åˆ¸å•†é­ä¸»ç®¡æ©Ÿé—œè£ç½°ã€Œè­¦å‘Šã€è™•åˆ†ï¼Œæœ‰å“ªäº›æ¥­å‹™æœƒå—é™åˆ¶ï¼Ÿ",
    "å…§ç·šäº¤æ˜“æœ‰ç½ªåˆ¤æ±ºæ‰€èªå®šé‡å¤§è¨Šæ¯æˆç«‹çš„æ™‚é»",
]


def get_system_prompt(selected_stores: List[str]) -> str:
    """
    æ ¹æ“šé¸å–çš„ Store çµ„åˆç”¢ç”Ÿç³»çµ±æç¤º
    """
    base_prompt = """ä½ æ˜¯å°ˆæ¥­çš„é‡‘èæ³•è¦é¡§å•ã€‚è«‹æ ¹æ“šåƒè€ƒè³‡æ–™å›ç­”å•é¡Œã€‚

å›ç­”æ™‚å¿…é ˆï¼š
1. æ˜ç¢ºå¼•ç”¨ä¾†æºæ–‡ä»¶ï¼ˆæª”æ¡ˆåç¨±ã€æ—¥æœŸï¼‰
2. å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦èªªæ˜
3. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
4. ä¿æŒå°ˆæ¥­ã€å®¢è§€çš„æ…‹åº¦
"""

    # æ ¹æ“šé¸å–çš„ Store åŠ å…¥ç‰¹å®šæŒ‡å¼•
    specific_guidelines = []

    if 'penalties' in selected_stores:
        specific_guidelines.append("""
ã€è£ç½°æ¡ˆä»¶æŒ‡å¼•ã€‘
- åˆ—èˆ‰å…·é«”æ¡ˆä¾‹èˆ‡è£ç½°å…§å®¹
- èªªæ˜å—ç½°æ©Ÿæ§‹ã€ç½°æ¬¾é‡‘é¡ã€é•è¦è¡Œç‚º
- å¼•ç”¨ç›¸é—œæ³•å¾‹ä¾æ“š""")

    if 'law_interpretations' in selected_stores:
        specific_guidelines.append("""
ã€æ³•ä»¤å‡½é‡‹æŒ‡å¼•ã€‘
- è§£é‡‹æ³•è¦çš„å…·é«”å«ç¾©
- åˆ—å‡ºä¿®æ­£å‰å¾Œçš„å·®ç•°ï¼ˆå¦‚æœ‰ï¼‰
- å¼•ç”¨ç™¼æ–‡å­—è™Ÿ""")

    if 'announcements' in selected_stores:
        specific_guidelines.append("""
ã€é‡è¦å…¬å‘ŠæŒ‡å¼•ã€‘
- èªªæ˜å…¬å‘Šçš„ä¸»è¦å…§å®¹
- åˆ—å‡ºç”Ÿæ•ˆæ—¥æœŸï¼ˆå¦‚æœ‰ï¼‰
- å¼•ç”¨å…¬å‘Šæ–‡è™Ÿ""")

    # çµ„åˆæç¤º
    if specific_guidelines:
        return base_prompt + "\n" + "\n".join(specific_guidelines)
    return base_prompt


def query_gemini(question: str, selected_stores: List[str], api_key: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨ Gemini File Search åŸ·è¡ŒæŸ¥è©¢
    """
    from google import genai
    from google.genai import types

    client = genai.Client(api_key=api_key)

    # å–å¾—é¸å–çš„ Store IDs
    store_ids = [STORES[s]['store_id'] for s in selected_stores if s in STORES]

    if not store_ids:
        return {
            'answer': 'è«‹è‡³å°‘é¸æ“‡ä¸€å€‹è³‡æ–™ä¾†æº',
            'sources': [],
            'error': True
        }

    # å–å¾—ç³»çµ±æç¤º
    system_prompt = get_system_prompt(selected_stores)

    start_time = time.time()

    try:
        # åŸ·è¡ŒæŸ¥è©¢
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=question,
            config=types.GenerateContentConfig(
                tools=[
                    types.Tool(
                        file_search=types.FileSearch(
                            file_search_store_names=store_ids
                        )
                    )
                ],
                temperature=0.1,
                max_output_tokens=2000,
                system_instruction=system_prompt
            )
        )

        latency = time.time() - start_time

        # æå–ç­”æ¡ˆ
        answer = response.text if hasattr(response, 'text') else str(response)

        # æå–ä¾†æº
        sources = extract_sources(response)

        return {
            'answer': answer,
            'sources': sources,
            'latency': latency,
            'error': False
        }

    except Exception as e:
        return {
            'answer': f'æŸ¥è©¢å¤±æ•—: {str(e)}',
            'sources': [],
            'error': True
        }


def format_source_display_name(raw_name: str) -> str:
    """
    å°‡åŸå§‹æª”æ¡ˆåç¨±æ ¼å¼åŒ–ç‚ºæ˜“è®€çš„é¡¯ç¤ºåç¨±

    åŸå§‹æ ¼å¼ç¯„ä¾‹ï¼š
    - æ³•ä»¤å‡½é‡‹: 2006-03-03_securities_bureau_law_amendment_fsc_law_201406240001
    - é‡è¦å…¬å‘Š: 2019-01-02_insurance_bureau_ann_amendment_fsc_unk_20190102_1648
    - è£ç½°æ¡ˆä»¶: 2025-09-25_insurance_bureau_penalty_fsc_pen_20250925_0001

    è¼¸å‡ºæ ¼å¼ï¼š
    - æ³•ä»¤å‡½é‡‹_2006-03-03
    - é‡è¦å…¬å‘Š_2019-01-02
    - è£ç½°æ¡ˆä»¶_2025-09-25
    """
    if not raw_name:
        return "æœªçŸ¥æ–‡ä»¶"

    # åˆ¤æ–·ä¾†æºé¡å‹
    source_type = "æœªçŸ¥"
    if 'fsc_law' in raw_name or 'law_' in raw_name:
        source_type = "æ³•ä»¤å‡½é‡‹"
    elif 'fsc_unk' in raw_name or 'ann_' in raw_name:
        source_type = "é‡è¦å…¬å‘Š"
    elif 'fsc_pen' in raw_name or 'penalty' in raw_name:
        source_type = "è£ç½°æ¡ˆä»¶"

    # æå–æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
    date = "æœªçŸ¥æ—¥æœŸ"
    parts = raw_name.split('_')
    if parts and len(parts[0]) == 10 and '-' in parts[0]:
        date = parts[0]

    return f"{source_type}_{date}"


def extract_sources(response) -> List[Dict[str, Any]]:
    """
    å¾ Gemini å›æ‡‰ä¸­æå–ä¾†æº
    """
    sources = []

    try:
        if hasattr(response, 'candidates') and len(response.candidates) > 0:
            candidate = response.candidates[0]

            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                metadata = candidate.grounding_metadata

                if hasattr(metadata, 'grounding_chunks') and metadata.grounding_chunks:
                    for i, chunk in enumerate(metadata.grounding_chunks):
                        if hasattr(chunk, 'retrieved_context'):
                            context = chunk.retrieved_context

                            # æå–åŸå§‹æª”å/ID
                            raw_id = ""
                            if hasattr(context, 'title') and context.title:
                                raw_id = context.title
                            elif hasattr(context, 'uri') and context.uri:
                                raw_id = context.uri.split('/')[-1]

                            # ä½¿ç”¨ mapping è§£æé¡¯ç¤ºåç¨±
                            display_name, source_type, date, original_url = resolve_source_display_name(raw_id)

                            snippet = ""
                            if hasattr(context, 'text') and context.text:
                                snippet = context.text[:500]

                            score = 1.0
                            if hasattr(chunk, 'score'):
                                score = float(chunk.score)

                            sources.append({
                                'filename': display_name,
                                'raw_id': raw_id,
                                'source_type': source_type,
                                'date': date,
                                'snippet': snippet,
                                'score': score,
                                'original_url': original_url,
                            })

    except Exception as e:
        st.warning(f"æå–ä¾†æºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return sources


def render_sidebar():
    """æ¸²æŸ“å´é‚Šæ¬„"""
    with st.sidebar:
        st.header("ğŸ“Š è³‡æ–™ä¾†æº")

        # Store é¸æ“‡ï¼ˆå¤šé¸ï¼‰
        selected_stores = []
        for key, store in STORES.items():
            checked = st.checkbox(
                f"{store['icon']} {store['display_name']}",
                value=(key == 'penalties'),  # é è¨­é¸å–è£ç½°æ¡ˆä»¶
                key=f"store_{key}",
                help=store['description']
            )
            if checked:
                selected_stores.append(key)

        st.markdown("---")

        # é¡¯ç¤ºé¸å–çš„è³‡æ–™çµ±è¨ˆ
        if selected_stores:
            total_docs = sum(STORES[s]['count'] for s in selected_stores)
            st.metric("ğŸ“š æ–‡ä»¶ç¸½æ•¸", f"{total_docs:,}")

            with st.expander("â„¹ï¸ è³‡æ–™èªªæ˜", expanded=False):
                for key in selected_stores:
                    store = STORES[key]
                    st.caption(f"{store['icon']} **{store['display_name']}**")
                    st.caption(f"  {store['description']}")
                    st.caption(f"  å…± {store['count']:,} ç­†")
        else:
            st.warning("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹è³‡æ–™ä¾†æº")

        st.markdown("---")

        # ä½¿ç”¨èªªæ˜
        with st.expander("ğŸ’¡ ä½¿ç”¨èªªæ˜", expanded=False):
            st.markdown("""
            **å¦‚ä½•ä½¿ç”¨ï¼š**
            1. åœ¨å·¦å´é¸æ“‡è¦æŸ¥è©¢çš„è³‡æ–™ä¾†æº
            2. è¼¸å…¥æ‚¨çš„å•é¡Œ
            3. é»æ“Šã€ŒæŸ¥è©¢ã€æŒ‰éˆ•
            4. æŸ¥çœ‹ AI ç”Ÿæˆçš„ç­”æ¡ˆå’Œåƒè€ƒä¾†æº

            **è³‡æ–™ä¾†æºèªªæ˜ï¼š**
            - **è£ç½°æ¡ˆä»¶**ï¼šé‡‘èæ©Ÿæ§‹é•è¦è£ç½°è¨˜éŒ„
            - **æ³•ä»¤å‡½é‡‹**ï¼šæ³•è¦è§£é‡‹å’Œä¿®æ­£èªªæ˜
            - **é‡è¦å…¬å‘Š**ï¼šé‡‘ç®¡æœƒæ”¿ç­–å…¬å‘Š
            """)

        st.markdown("---")
        st.caption("ğŸ¤– AI æ™ºèƒ½å•ç­”ç³»çµ±")
        st.caption("âš ï¸ æœ¬ç³»çµ±åƒ…ä¾›åƒè€ƒ")

    return selected_stores


def main():
    """ä¸»ç¨‹å¼"""
    # å–å¾— API Key
    api_key = st.secrets.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY")

    if not api_key:
        st.error("è«‹è¨­å®š GEMINI_API_KEY")
        st.stop()

    # æ¸²æŸ“å´é‚Šæ¬„
    selected_stores = render_sidebar()

    # ä¸»æ¨™é¡Œ
    st.title("ğŸ›ï¸ é‡‘ç®¡æœƒæ™ºèƒ½å•ç­”")
    st.caption("ğŸ’¡ æœ¬ç³»çµ±ç‚ºå±•ç¤ºç”¨ï¼Œå¦‚é‡ç•«é¢ç„¡åæ‡‰ï¼Œè«‹é‡æ–°æ•´ç†é é¢")

    # å•é¡Œè¼¸å…¥
    if 'current_question' not in st.session_state:
        st.session_state.current_question = ""

    question = st.text_area(
        "è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š",
        value=st.session_state.current_question,
        placeholder="ä¾‹å¦‚ï¼šå“ªäº›éŠ€è¡Œå› ç‚ºç†å°ˆæŒªç”¨å®¢æˆ¶æ¬¾é …è¢«è£ç½°ï¼Ÿ",
        height=100
    )

    # æ›´æ–° session state
    if question != st.session_state.current_question:
        st.session_state.current_question = question

    # æŒ‰éˆ•åˆ—
    col1, col2, col3 = st.columns([1, 1, 4])

    with col1:
        submit_button = st.button("ğŸ” æŸ¥è©¢", type="primary", use_container_width=True)

    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤", use_container_width=True):
            st.session_state.current_question = ""
            st.rerun()

    # è™•ç†æŸ¥è©¢
    if submit_button and question:
        if not selected_stores:
            st.error("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹è³‡æ–™ä¾†æº")
        else:
            with st.spinner("ğŸ” AI æŸ¥è©¢ä¸­..."):
                result = query_gemini(question, selected_stores, api_key)

            if result['error']:
                st.error(result['answer'])
            else:
                # é¡¯ç¤ºçµæœ
                st.success("âœ… æŸ¥è©¢å®Œæˆ")

                # æŒ‡æ¨™æ¬„ï¼ˆä½¿ç”¨è¼ƒå°å­—é«”ï¼‰
                stores_text = ", ".join([STORES[s]['display_name'] for s in selected_stores])
                st.caption(f"â±ï¸ å›æ‡‰æ™‚é–“: {result['latency']:.2f} ç§’ã€€ï½œã€€ğŸ“š ä¾†æºæ•¸é‡: {len(result['sources'])} ç­†ã€€ï½œã€€ğŸ“‚ æŸ¥è©¢ç¯„åœ: {stores_text}")

                st.markdown("---")

                # ç­”æ¡ˆ
                st.subheader("ğŸ“ ç­”æ¡ˆ")
                st.markdown(result['answer'])

                st.markdown("---")

                # ä¾†æºï¼ˆæŒ‰é¡å‹åˆ†çµ„ï¼Œå„çµ„æŒ‰æ™‚é–“æ’åºï¼‰
                if result['sources']:
                    st.subheader(f"ğŸ“š åƒè€ƒä¾†æº ({len(result['sources'])} ç­†)")

                    # æŒ‰é¡å‹åˆ†çµ„
                    penalties = [s for s in result['sources'] if s.get('source_type') == "è£ç½°æ¡ˆä»¶"]
                    law_interps = [s for s in result['sources'] if s.get('source_type') == "æ³•ä»¤å‡½é‡‹"]
                    announcements = [s for s in result['sources'] if s.get('source_type') == "é‡è¦å…¬å‘Š"]
                    others = [s for s in result['sources'] if s.get('source_type') not in ["è£ç½°æ¡ˆä»¶", "æ³•ä»¤å‡½é‡‹", "é‡è¦å…¬å‘Š"]]

                    # å„çµ„æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°åˆ°æœ€èˆŠï¼‰
                    for group in [penalties, law_interps, announcements, others]:
                        group.sort(key=lambda x: x.get('date', ''), reverse=True)

                    # ä¾åºé¡¯ç¤ºï¼šè£ç½° â†’ å‡½é‡‹ â†’ å…¬å‘Š â†’ å…¶ä»–
                    type_config = [
                        ("âš–ï¸", "è£ç½°æ¡ˆä»¶", penalties),
                        ("ğŸ“œ", "æ³•ä»¤å‡½é‡‹", law_interps),
                        ("ğŸ“¢", "é‡è¦å…¬å‘Š", announcements),
                        ("ğŸ“„", "å…¶ä»–", others),
                    ]

                    for icon, type_name, sources_list in type_config:
                        if not sources_list:
                            continue

                        st.caption(f"{icon} {type_name} ({len(sources_list)} ç­†)")
                        for source in sources_list:
                            with st.expander(
                                f"{icon} {source['filename']}",
                                expanded=False
                            ):
                                st.markdown(f"**ç›¸é—œå…§å®¹ï¼š**")
                                st.markdown(f"> {source['snippet'][:300]}...")

                                if source['score'] < 1.0:
                                    st.caption(f"ç›¸ä¼¼åº¦: {source['score']:.2%}")

                                # é¡¯ç¤ºåŸå§‹ç¶²é é€£çµ
                                if source.get('original_url'):
                                    st.markdown(f"[ğŸ”— æŸ¥çœ‹åŸå§‹ç¶²é ]({source['original_url']})")
                else:
                    # sources=0 è‡ªå‹•é‡è©¦
                    st.warning("âš ï¸ æœªæ‰¾åˆ°åƒè€ƒä¾†æºï¼Œæ­£åœ¨é‡è©¦...")
                    with st.spinner("é‡æ–°æŸ¥è©¢ä¸­..."):
                        result2 = query_gemini(question, selected_stores, api_key)

                    if result2['sources']:
                        st.success("âœ… é‡è©¦æˆåŠŸ")
                        st.markdown(result2['answer'])

                        # æŒ‰é¡å‹åˆ†çµ„ä¸¦æ’åº
                        penalties2 = sorted([s for s in result2['sources'] if s.get('source_type') == "è£ç½°æ¡ˆä»¶"], key=lambda x: x.get('date', ''), reverse=True)
                        law_interps2 = sorted([s for s in result2['sources'] if s.get('source_type') == "æ³•ä»¤å‡½é‡‹"], key=lambda x: x.get('date', ''), reverse=True)
                        announcements2 = sorted([s for s in result2['sources'] if s.get('source_type') == "é‡è¦å…¬å‘Š"], key=lambda x: x.get('date', ''), reverse=True)
                        others2 = sorted([s for s in result2['sources'] if s.get('source_type') not in ["è£ç½°æ¡ˆä»¶", "æ³•ä»¤å‡½é‡‹", "é‡è¦å…¬å‘Š"]], key=lambda x: x.get('date', ''), reverse=True)

                        for icon, type_name, sources_list in [("âš–ï¸", "è£ç½°æ¡ˆä»¶", penalties2), ("ğŸ“œ", "æ³•ä»¤å‡½é‡‹", law_interps2), ("ğŸ“¢", "é‡è¦å…¬å‘Š", announcements2), ("ğŸ“„", "å…¶ä»–", others2)]:
                            if not sources_list:
                                continue
                            st.caption(f"{icon} {type_name} ({len(sources_list)} ç­†)")
                            for source in sources_list:
                                with st.expander(f"{icon} {source['filename']}"):
                                    st.markdown(f"> {source['snippet'][:300]}...")
                                    if source.get('original_url'):
                                        st.markdown(f"[ğŸ”— æŸ¥çœ‹åŸå§‹ç¶²é ]({source['original_url']})")
                    else:
                        st.info("ä½ æŸ¥è©¢çš„å•é¡Œåœ¨ç›®å‰çš„æ–‡ä»¶åº«ä¸­æ²’æœ‰åˆé©çš„çµæœï¼Œè«‹å˜—è©¦æ›å€‹æ–¹å¼æè¿°æ‚¨çš„å•é¡Œã€‚")

    # ç¯„ä¾‹å•é¡Œ
    if not question:
        st.markdown("---")
        st.subheader("ğŸ’¡ ç¯„ä¾‹å•é¡Œ")

        cols = st.columns(2)
        for idx, eq in enumerate(EXAMPLE_QUESTIONS):
            col = cols[idx % 2]
            with col:
                if st.button(f"ğŸ“Œ {eq}", key=f"example_{idx}", use_container_width=True):
                    st.session_state.current_question = eq
                    st.rerun()


if __name__ == "__main__":
    main()
